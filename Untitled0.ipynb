{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNfEBugQN85XMXsSUW6i5kG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZd1rgNIqRsZ","outputId":"2505cac4-0f8c-4d7b-e2b3-1c4451cb7a8e","executionInfo":{"status":"ok","timestamp":1743038921193,"user_tz":-540,"elapsed":19394,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcp5EiRJqtpE","outputId":"007ede12-4114-45c3-84d5-bea22c261eef","executionInfo":{"status":"ok","timestamp":1743038929641,"user_tz":-540,"elapsed":6,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/gradient_accumulation\n"]}],"source":["%cd drive/MyDrive/Colab Notebooks/gradient_accumulation/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMToGg-drbvk","outputId":"aecd295d-91f9-4ecf-b406-a5417ba67c97","executionInfo":{"status":"ok","timestamp":1743038931172,"user_tz":-540,"elapsed":204,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["data  __pycache__  trainer.py  Untitled0.ipynb\tvit_train.py\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KHcdOs_jqvXX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743039030595,"user_tz":-540,"elapsed":97612,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}},"outputId":"e430a32c-de59-4e30-c8d3-3aff6a218b22"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install timm"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXpTAhOu6Yx1","outputId":"d153c2d5-4fab-4340-f7cd-99c2aae212bd","executionInfo":{"status":"ok","timestamp":1743041823613,"user_tz":-540,"elapsed":392930,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","Class: 10\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/trainer.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=use_amp):\n","100% 79/79 [00:20<00:00,  3.81it/s]\n","epoch: 1, train loss: 0.4662214268189974, train accuracy: 0.84336, test loss: 0.1470433767837814, test accuracy: 0.9497\n","100% 79/79 [00:18<00:00,  4.32it/s]\n","epoch: 2, train loss: 0.10657203677468575, train accuracy: 0.96194, test loss: 0.10741472619149504, test accuracy: 0.9665\n","100% 79/79 [00:19<00:00,  4.00it/s]\n","epoch: 3, train loss: 0.07104151334661321, train accuracy: 0.97442, test loss: 0.10245820418990488, test accuracy: 0.9654\n","Exception in thread Thread-8 (_pin_memory_loop):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n","    do_one_step()\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n","    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n","    return _ForkingPickler.loads(res)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n","    fd = df.detach()\n","         ^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/resource_sharer.py\", line 57, in detach\n","    with _resource_sharer.get_connection(self._id) as conn:\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/resource_sharer.py\", line 86, in get_connection\n","    c = Client(address, authkey=process.current_process().authkey)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 519, in Client\n","    c = SocketClient(address)\n","        ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 647, in SocketClient\n","    s.connect(address)\n","FileNotFoundError: [Errno 2] No such file or directory\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py\", line 127, in <module>\n","    main(args)\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py\", line 92, in main\n","    train_loss, train_count = trainer.train(device, train_loader, model, criterion, optimizer, lr_scheduler, \n","                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/trainer.py\", line 28, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n","    _engine_run_backward(\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n","    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]}],"source":["!python3 vit_train.py --epoch 10 --batch_size 128 --img_size 224 --amp --dataset cifar10 --warmup_t 0 --warmup_lr_init 0 --accumulation_steps 4 --max_grad_norm 1.0"]},{"cell_type":"code","source":["!python3 vit_train.py --epoch 10 --batch_size 128 --img_size 224 --amp --dataset cifar10 --warmup_t 0 --warmup_lr_init 0 --accumulation_steps 1 --max_grad_norm 1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GT14p8-QO3a","executionInfo":{"status":"ok","timestamp":1743041421544,"user_tz":-540,"elapsed":409314,"user":{"displayName":"Syunki T","userId":"06415123551730100088"}},"outputId":"a7efbc47-d1d2-4b73-cd72-cad7ea4d7d60"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","Class: 10\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n","/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/trainer.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=use_amp):\n","100% 79/79 [00:18<00:00,  4.23it/s]\n","epoch: 1, train loss: 0.27800855169502586, train accuracy: 0.90456, test loss: 0.1431396982243544, test accuracy: 0.9507\n","100% 79/79 [00:18<00:00,  4.35it/s]\n","epoch: 2, train loss: 0.11799305954422706, train accuracy: 0.95858, test loss: 0.11507274064270756, test accuracy: 0.9606\n","100% 79/79 [00:19<00:00,  4.12it/s]\n","epoch: 3, train loss: 0.08174619222632967, train accuracy: 0.9706, test loss: 0.12604400803205335, test accuracy: 0.9556\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py\", line 127, in <module>\n","    main(args)\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/vit_train.py\", line 92, in main\n","    train_loss, train_count = trainer.train(device, train_loader, model, criterion, optimizer, lr_scheduler, \n","                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/Colab Notebooks/gradient_accumulation/trainer.py\", line 28, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n","    _engine_run_backward(\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n","    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n"]}]}]}